![rate-limiter](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://github.com/AndreiYu/systems_design/blob/master/rate_limiter/system-design.puml)

======================
### Rate limiter types

| type                   | description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | +                                                                                                                                                                                            | -                                                                                                                                                                                                                                                                                                                                                                 |
|------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Token bucket           | A token bucket maintains a rolling and accumulating budget of usage as a balance of tokens. This technique recognizes that not all inputs to a service correspond 1:1 with requests. A token bucket adds tokens at some rate. When a service request is made, the service attempts to withdraw a token (decrementing the token count) to fulfill the request. If there are no tokens in the bucket, the service has reached its limit and responds with backpressure. For example, in a GraphQL service, a single request might result in multiple operations that are composed into a result. These operations may each take one token. This way, the service can keep track of the capacity that it needs to limit the use of, rather than tie the rate-limiting technique directly to requests. | * The algorithm is easy to implement.  </br>* Memory efficient.  </br>* Token bucket allows a burst of traffic for short periods. A request can go through as long as there are tokens left. | * Two parameters in the algorithm are bucket size and token refill rate. However, it might be challenging to tune them properly.                                                                                                                                                                                                                                  |
| Leaking bucket         | The leaking bucket algorithm is similar to the token bucket except that requests are processed at a fixed rate. It is usually implemented with a first-in-first-out (FIFO) queue. The algorithm works as follows: </br>* When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue.</br>* Otherwise, the request is dropped. </br>* Requests are pulled from the queue and processed at regular intervals.                                                                                                                                                                                                                                                                                                                              | * Memory efficient given the limited queue size. </br>* Requests are processed at a fixed rate therefore it is suitable for use cases that a stable outflow rate is needed.                  | * A burst of traffic fills up the queue with old requests, and if they are not processed in time, recent requests will be rate limited.</br>* There are two parameters in the algorithm. It might not be easy to tune them properly.                                                                                                                              |                                                                                                                                                                                              |                                                                                                                                  |
| Fixed window counter   | Fixed window counter algorithm works as follows: </br>* The algorithm divides the timeline into fix-sized time windows and assign a counter for each window. </br>* Each request increments the counter by one. </br>* Once the counter reaches the pre-defined threshold, new requests are dropped until a new time window starts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                | * Memory efficient. </br> *Easy to understand. </br>* Resetting available quota at the end of a unit time window fits certain use cases.                                                     | * Spike in traffic at the edges of a window could cause more requests than the allowed quota to go through.                                                                                                                                                                                                                                                       |
| Sliding window log     | The algorithm keeps track of request timestamps. Timestamp data is usually kept in cache, such as sorted sets of Redis. When a new request comes in, remove all the outdated timestamps. Outdated timestamps are defined as those older than the start of the current time window. Add timestamp of the new request to the log. If the log size is the same or lower than the allowed count, a request is accepted. Otherwise, it is rejected.                                                                                                                                                                                                                                                                                                                                                     | * Rate limiting implemented by this algorithm is very accurate. In any rolling window, requests will not exceed the rate limit.                                                              | * The algorithm consumes a lot of memory because even if a request is rejected, its timestamp might still be stored in memory                                                                                                                                                                                                                                     |
| Sliding window counter | The sliding window counter algorithm is a hybrid approach that combines the fixed window counter and sliding window log. The algorithm can be implemented by two different approaches: </br> * N of req in rolling window = Requests in current window + requests in the previous window * overlap percentage of the rolling window and previous window </br> * The entire window time is broken down into smaller buckets. The size of each bucket depends on how much elasticity is allowed for the rate limiter (for ex. 5 sec buckets for 1 minute window)                                                                                                                                                                                                                                     | * It smooths out spikes in traffic because the rate is based on the average rate of the previous window. </br>* Memory efficient.                                                            | *It only works for not-so-strict look back window. It is an approximation of the actual rate because it assumes requests in the previous window are evenly distributed. However, this problem may not be as bad as it seems. According to experiments done by Cloudflare, only 0.003% of requests are wrongly allowed or rate limited among 400 million requests. |

### Reference materials
1) [GCP Rate-limiting strategies and techniques](https://cloud.google.com/architecture/rate-limiting-strategies-techniques)
2) [AWS Api Gateway Requests throttling](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html)
3) [Stripe rate limiters](https://stripe.com/blog/rate-limiters)  
A good explanation of so-called "load shedding", when low-priority requests are dropped in order to make sure that more critical requests get through.  

<details open="">

| type                            | description                                                                                                                                                                                                                                                                                                                                                                                                                         |
|---------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Request                         | "N requests per second"                                                                                                                                                                                                                                                                                                                                                                                                             |
| Concurrent requests limiter     | "You can only have 20 API requests in progress at the same time".  <br/>It is completely reasonable to tune this limiter up so it rejects more often than the Request Rate Limiter. It asks your users to use a different programming model of “Fork off X jobs and have them process the queue” compared to “Hammer the API and back off when I get a HTTP 429”.                                                                   |
| Fleet usage load shedder        | divide up the traffic into two types: critical API methods (e.g. creating charges) and non-critical methods (e.g. listing charges.) There is a Redis cluster that counts how many requests we currently have of each type.  <br/>Reserve a fraction of the infrastructure for critical requests. If your reservation number is 20%, then any non-critical request over their 80% allocation would be rejected with status code 503. |
| Worker utilization load shedder | This load shedder is the final line of defense. If your workers start getting backed up with requests, then this will shed lower-priority traffic.  <br/>If a box is too busy to handle its request volume, it will slowly start shedding less-critical requests, starting with test mode traffic. If shedding test mode traffic gets it back into a good state, great! We can start to slowly bring traffic back                   |
[show me the code](https://gist.github.com/ptarjan/e38f45f2dfe601419ca3af937fff574d#request-rate-limiter)
</details>

4) [<b>Better Rate Limiting With Redis Sorted Sets</b>](https://engineering.classdojo.com/blog/2015/02/06/rolling-rate-limiter/) //TODO: add flow diagram for this case
5) [System Design — Rate limiter and Data modelling](https://medium.com/@saisandeepmopuri/system-design-rate-limiter-and-data-modelling-9304b0d18250)
6) [How we built rate limiting capable of scaling to millions of domains](https://blog.cloudflare.com/counting-things-a-lot-of-different-things/)
7) [Scaling your API with rate limiters](https://gist.github.com/ptarjan/e38f45f2dfe601419ca3af937fff574d#request-rate-limiter)
8) [Rate Limit Requests with Iptables](https://blog.programster.org/rate-limit-requests-with-iptables)
9) [OSI model](https://en.wikipedia.org/wiki/OSI_model#Layer_architecture)




